# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14T_yNsmXP1i5QPGy7stnoNLTvbMG3vUS
"""

# ------------------------------
# Optimized ATS System – Fully Automatic Colab Version
# ------------------------------

# 1️⃣ Install dependencies
!pip install flask flask-ngrok pymupdf python-docx scikit-learn sentence-transformers ipywidgets matplotlib wordcloud pyngrok

# 2️⃣ Imports
import os, re, json, sqlite3, fitz, docx, numpy as np
from flask import Flask, request, jsonify
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from ipywidgets import widgets, VBox, Output
from IPython.display import display
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from pyngrok import ngrok
import threading
import requests

# 3️⃣ Setup
app = Flask(__name__)
DB_NAME = "ats_auto.db"
os.makedirs("uploads", exist_ok=True)
embedder = SentenceTransformer("all-MiniLM-L6-v2")
SKILLS_LIST = ["python","java","sql","ml","machine learning","cloud","docker","kubernetes",
               "excel","c++","tensorflow","pytorch","aws","azure","linux","git"]

# 4️⃣ Initialize DB
def init_db():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""CREATE TABLE IF NOT EXISTS job_description (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        jd_text TEXT,
                        jd_emb BLOB
                     )""")
        c.execute("""CREATE TABLE IF NOT EXISTS evaluations (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        resume_name TEXT,
                        score REAL,
                        verdict TEXT,
                        missing_skills TEXT,
                        feedback TEXT
                     )""")
init_db()

# 5️⃣ Text extractors
def extract_text(file_path):
    if file_path.lower().endswith(".pdf"):
        doc = fitz.open(file_path)
        return " ".join([page.get_text("text") for page in doc])
    elif file_path.lower().endswith(".docx"):
        return " ".join([p.text for p in docx.Document(file_path).paragraphs])
    else:
        raise ValueError("Unsupported format")

def clean_text(text):
    return re.sub(r'\s+', ' ', text).strip().lower()

# 6️⃣ Resume parser
def parse_resume(file_path):
    text = clean_text(extract_text(file_path))
    skills = [s for s in SKILLS_LIST if s in text]
    education = " ".join(re.findall(r"(education|bachelor|master|degree|university|college).*", text))
    experience = " ".join(re.findall(r"(experience|worked|intern|project|developed).*", text))
    projects = " ".join(re.findall(r"(project|built|developed|created).*", text))
    return {"Education":education,"Skills":" ".join(skills),"Experience":experience,"Projects":projects,"FullText":text}

# 7️⃣ JD parser
def parse_jd(jd_text):
    jd_text = clean_text(jd_text)
    jd_json = {"RoleTitle":"","MustHaveSkills":[],"GoodToHaveSkills":[],"Qualifications":[],"FullText":jd_text}
    for line in jd_text.split("\n"):
        l = line.lower()
        if "role" in l or "title" in l: jd_json["RoleTitle"]=line.strip()
        elif "must" in l or "required" in l: jd_json["MustHaveSkills"].append(line.strip())
        elif "good" in l or "preferred" in l: jd_json["GoodToHaveSkills"].append(line.strip())
        elif "qualification" in l or "degree" in l: jd_json["Qualifications"].append(line.strip())
    return jd_json

# 8️⃣ Evaluation function
def evaluate_resume(resume_text, jd_text, jd_emb=None, weight_keywords=0.5, weight_semantics=0.5):
    resume_words = set(resume_text.split())
    jd_words = set(jd_text.split())
    keyword_score = len(resume_words & jd_words)/max(len(jd_words),1)
    if jd_emb is None: jd_emb = embedder.encode([jd_text])
    resume_emb = embedder.encode([resume_text])
    semantic_score = float(cosine_similarity([resume_emb[0]],[jd_emb[0]])[0][0])
    final_score = (keyword_score*weight_keywords + semantic_score*weight_semantics)*100
    missing_skills = [s for s in SKILLS_LIST if s in jd_text and s not in resume_text]
    verdict = "High" if final_score>=75 else "Medium" if final_score>=50 else "Low"
    feedback = f"Score: {final_score:.1f}. "+("✅ Great match!" if verdict=="High" else "⚠️ Partial match." if verdict=="Medium" else "❌ Low alignment.")
    if missing_skills: feedback += f" Missing: {', '.join(missing_skills[:10])}."
    return final_score, verdict, missing_skills, feedback

# 9️⃣ Visualizer
def visualize_resume(resume_json, missing_skills):
    matched_skills = [s for s in resume_json["Skills"].split() if s not in missing_skills]
    plt.figure(figsize=(6,4))
    plt.bar(["Matched Skills","Missing Skills"], [len(matched_skills), len(missing_skills)], color=["green","red"])
    plt.title("Skills Match")
    plt.show()
    wc = WordCloud(width=800,height=400,background_color='white').generate(resume_json["FullText"])
    plt.figure(figsize=(15,5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title("Resume Word Cloud")
    plt.show()

# 10️⃣ Flask API
@app.route("/upload_jd", methods=["POST"])
def upload_jd():
    jd_text = request.get_json().get("jd_text","")
    if not jd_text: return jsonify({"error":"JD text required"}),400
    jd_emb = embedder.encode([jd_text])
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("INSERT INTO job_description (jd_text,jd_emb) VALUES (?,?)",(jd_text,jd_emb.tobytes()))
        conn.commit()
    return jsonify({"message":"JD uploaded successfully"})

@app.route("/upload_resume", methods=["POST"])
def upload_resume():
    file = request.files.get("file")
    if not file: return jsonify({"error":"No file uploaded"}),400
    path = os.path.join("uploads",file.filename)
    file.save(path)
    resume_json = parse_resume(path)
    resume_text = resume_json["FullText"]
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("SELECT jd_text,jd_emb FROM job_description ORDER BY id DESC LIMIT 1")
        row = c.fetchone()
    if not row: return jsonify({"error":"No JD uploaded"}),400
    jd_text = row[0]
    jd_emb = np.frombuffer(row[1],dtype=np.float32).reshape(1,-1)
    score, verdict, missing_skills, feedback = evaluate_resume(resume_text,jd_text,jd_emb=jd_emb)
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("INSERT INTO evaluations (resume_name,score,verdict,missing_skills,feedback) VALUES (?,?,?,?,?)",
                  (file.filename,score,verdict,json.dumps(missing_skills),feedback))
        conn.commit()
    return jsonify({"resume":file.filename,"score":round(score,2),"verdict":verdict,
                     "missing_skills":missing_skills,"feedback":feedback})

@app.route("/get_results", methods=["GET"])
def get_results():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("SELECT resume_name,score,verdict,missing_skills,feedback FROM evaluations")
        rows = c.fetchall()
    results = [{"resume":r[0],"score":round(r[1],2),"verdict":r[2],
                "missing_skills":json.loads(r[3]),"feedback":r[4]} for r in rows]
    return jsonify(results)

# 11️⃣ Start Flask in a thread
def start_flask():
    app.run()

threading.Thread(target=start_flask).start()

# 12️⃣ Open ngrok tunnel automatically
public_url = ngrok.connect(5000).public_url
print("Your ATS server is running at:", public_url)

# 13️⃣ Colab Front-End
out = Output()
example_jd = """Role Title: Data Scientist
Must-Have Skills:
- Python, SQL, Machine Learning
Good-to-Have Skills:
- Deep Learning, TensorFlow, PyTorch
Qualifications:
- Bachelor or Master in CS, Statistics, or related field
"""
jd_text_area = widgets.Textarea(value=example_jd,layout=widgets.Layout(width='90%',height='150px'))
resume_upload = widgets.FileUpload(accept=".pdf,.docx", multiple=False)
submit_btn = widgets.Button(description="Submit & Evaluate")

def on_submit_visual(b):
    out.clear_output()
    if not resume_upload.value:
        with out: print("Upload a resume first."); return
    resume_filename = list(resume_upload.value.keys())[0]
    resume_bytes = resume_upload.value[resume_filename]['content']
    # Upload JD
    requests.post(f"{public_url}/upload_jd", json={"jd_text": jd_text_area.value})
    # Upload Resume
    files = {"file":(resume_filename,resume_bytes)}
    res = requests.post(f"{public_url}/upload_resume", files=files)
    result = res.json()
    with out: print(json.dumps(result, indent=4))
    # Visuals
    path = os.path.join("uploads",resume_filename)
    with open(path,"wb") as f: f.write(resume_bytes)
    resume_json = parse_resume(path)
    visualize_resume(resume_json, result["missing_skills"])

submit_btn.on_click(on_submit_visual)
display(VBox([jd_text_area,resume_upload,submit_btn,out]))

from pyngrok import ngrok

# Replace with your ngrok authtoken
NGROK_AUTH_TOKEN = "32yUnymxDwmFpTZBrB5Z2d9VVgR_57S2J5qjL2Zyy3cxoy1dW"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Start tunnel
public_url = ngrok.connect(5000).public_url
print("Your ATS server is running at:", public_url)

import glob

resume_files = glob.glob("/content/resumes/*")  # all PDF/DOCX files

for resume_path in resume_files:
    resume_json = parse_resume(resume_path)
    score, verdict, missing_skills, feedback = evaluate_resume(resume_json["FullText"], jd_text)
    print(resume_path)
    print(f"Score: {score:.2f}, Verdict: {verdict}, Missing Skills: {missing_skills}, Feedback: {feedback}\n")

# -----------------------------
# Complete ATS System – Single & Batch Evaluation (Optimized)
# -----------------------------

# 1️⃣ Install dependencies
!pip install flask flask-ngrok pymupdf python-docx scikit-learn sentence-transformers ipywidgets matplotlib wordcloud pyngrok

# 2️⃣ Imports
import os, re, json, sqlite3, fitz, docx, numpy as np, glob, threading
from flask import Flask, request, jsonify
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from ipywidgets import widgets, VBox, Output
from IPython.display import display
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from pyngrok import ngrok
import requests
import pandas as pd
from google.colab import files

# 3️⃣ Setup folders and constants
app = Flask(__name__)
DB_NAME = "ats_auto.db"
uploads_dir = "uploads"
resumes_dir = "resumes"
os.makedirs(uploads_dir, exist_ok=True)
os.makedirs(resumes_dir, exist_ok=True)

embedder = SentenceTransformer("all-MiniLM-L6-v2")
SKILLS_LIST = ["python","java","sql","ml","machine learning","cloud","docker","kubernetes",
               "excel","c++","tensorflow","pytorch","aws","azure","linux","git"]

# 4️⃣ Initialize DB
def init_db():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("""CREATE TABLE IF NOT EXISTS job_description (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        jd_text TEXT,
                        jd_emb BLOB
                     )""")
        c.execute("""CREATE TABLE IF NOT EXISTS evaluations (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        resume_name TEXT,
                        score REAL,
                        verdict TEXT,
                        missing_skills TEXT,
                        feedback TEXT
                     )""")
init_db()

# 5️⃣ Text extraction and cleaning
def extract_text(file_path):
    if file_path.lower().endswith(".pdf"):
        doc = fitz.open(file_path)
        return " ".join([page.get_text("text") for page in doc])
    elif file_path.lower().endswith(".docx"):
        return " ".join([p.text for p in docx.Document(file_path).paragraphs])
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

def clean_text(text):
    return re.sub(r'\s+', ' ', text).strip().lower()

# 6️⃣ Resume parser
def parse_resume(file_path):
    text = clean_text(extract_text(file_path))
    skills = [s for s in SKILLS_LIST if s in text]
    education = " ".join(re.findall(r"(education|bachelor|master|degree|university|college).*", text))
    experience = " ".join(re.findall(r"(experience|worked|intern|project|developed).*", text))
    projects = " ".join(re.findall(r"(project|built|developed|created).*", text))
    return {"Education":education,"Skills":" ".join(skills),"Experience":experience,"Projects":projects,"FullText":text}

# 7️⃣ Evaluation function
def evaluate_resume(resume_text, jd_text, jd_emb=None, weight_keywords=0.5, weight_semantics=0.5):
    resume_words = set(resume_text.split())
    jd_words = set(jd_text.split())
    keyword_score = len(resume_words & jd_words)/max(len(jd_words),1)
    if jd_emb is None: jd_emb = embedder.encode([jd_text])
    resume_emb = embedder.encode([resume_text])
    semantic_score = float(cosine_similarity([resume_emb[0]],[jd_emb[0]])[0][0])
    final_score = (keyword_score*weight_keywords + semantic_score*weight_semantics)*100
    missing_skills = [s for s in SKILLS_LIST if s in jd_text and s not in resume_text]
    verdict = "High" if final_score>=75 else "Medium" if final_score>=50 else "Low"
    feedback = f"Score: {final_score:.1f}. "+("✅ Great match!" if verdict=="High" else "⚠️ Partial match." if verdict=="Medium" else "❌ Low alignment.")
    if missing_skills: feedback += f" Missing: {', '.join(missing_skills[:10])}."
    return final_score, verdict, missing_skills, feedback

# 8️⃣ Visualizer
def visualize_resume(resume_json, missing_skills):
    matched_skills = [s for s in resume_json["Skills"].split() if s not in missing_skills]
    plt.figure(figsize=(6,4))
    plt.bar(["Matched Skills","Missing Skills"], [len(matched_skills), len(missing_skills)], color=["green","red"])
    plt.title("Skills Match")
    plt.show()
    wc = WordCloud(width=800,height=400,background_color='white').generate(resume_json["FullText"])
    plt.figure(figsize=(15,5))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title("Resume Word Cloud")
    plt.show()

# 9️⃣ Flask APIs
@app.route("/upload_jd", methods=["POST"])
def upload_jd():
    jd_text = request.get_json().get("jd_text","")
    if not jd_text: return jsonify({"error":"JD text required"}),400
    jd_emb = embedder.encode([jd_text])
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("INSERT INTO job_description (jd_text,jd_emb) VALUES (?,?)",(jd_text,jd_emb.tobytes()))
        conn.commit()
    return jsonify({"message":"JD uploaded successfully"})

@app.route("/upload_resume", methods=["POST"])
def upload_resume():
    file = request.files.get("file")
    if not file: return jsonify({"error":"No file uploaded"}),400
    path = os.path.join(uploads_dir,file.filename)
    file.save(path)
    resume_json = parse_resume(path)
    resume_text = resume_json["FullText"]
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("SELECT jd_text,jd_emb FROM job_description ORDER BY id DESC LIMIT 1")
        row = c.fetchone()
    if not row: return jsonify({"error":"No JD uploaded"}),400
    jd_text = row[0]
    jd_emb = np.frombuffer(row[1],dtype=np.float32).reshape(1,-1)
    score, verdict, missing_skills, feedback = evaluate_resume(resume_text,jd_text,jd_emb=jd_emb)
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("INSERT INTO evaluations (resume_name,score,verdict,missing_skills,feedback) VALUES (?,?,?,?,?)",
                  (file.filename,score,verdict,json.dumps(missing_skills),feedback))
        conn.commit()
    return jsonify({"resume":file.filename,"score":round(score,2),"verdict":verdict,
                     "missing_skills":missing_skills,"feedback":feedback})

@app.route("/get_results", methods=["GET"])
def get_results():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("SELECT resume_name,score,verdict,missing_skills,feedback FROM evaluations")
        rows = c.fetchall()
    results = [{"resume":r[0],"score":round(r[1],2),"verdict":r[2],
                "missing_skills":json.loads(r[3]),"feedback":r[4]} for r in rows]
    return jsonify(results)

# 10️⃣ Start Flask + Ngrok
def start_flask():
    app.run()
threading.Thread(target=start_flask).start()
public_url = ngrok.connect(5000).public_url
print("ATS server running at:", public_url)

# 11️⃣ Colab front-end – Single Resume
out = Output()
example_jd = """Role Title: Data Scientist
Must-Have Skills:
- Python, SQL, Machine Learning
Good-to-Have Skills:
- Deep Learning, TensorFlow, PyTorch
Qualifications:
- Bachelor or Master in CS, Statistics, or related field
"""
jd_text_area = widgets.Textarea(value=example_jd,layout=widgets.Layout(width='90%',height='150px'))
resume_upload = widgets.FileUpload(accept=".pdf,.docx", multiple=False)
submit_btn = widgets.Button(description="Submit & Evaluate")

def on_submit_visual(b):
    out.clear_output()
    if not resume_upload.value:
        with out: print("Upload a resume first."); return
    resume_filename = list(resume_upload.value.keys())[0]
    resume_bytes = resume_upload.value[resume_filename]['content']
    # Upload JD
    requests.post(f"{public_url}/upload_jd", json={"jd_text": jd_text_area.value})
    # Upload Resume
    files_dict = {"file":(resume_filename,resume_bytes)}
    res = requests.post(f"{public_url}/upload_resume", files=files_dict)
    result = res.json()
    with out: print(json.dumps(result, indent=4))
    # Visuals
    path = os.path.join(uploads_dir,resume_filename)
    with open(path,"wb") as f: f.write(resume_bytes)
    resume_json = parse_resume(path)
    visualize_resume(resume_json, result["missing_skills"])

submit_btn.on_click(on_submit_visual)
display(VBox([jd_text_area,resume_upload,submit_btn,out]))

# 12️⃣ Batch Evaluation – Optimized Precomputed Embeddings
print("\n--- Batch Evaluation ---")
uploaded_resumes = files.upload()
for fname in uploaded_resumes.keys():
    with open(os.path.join(resumes_dir, fname), "wb") as f:
        f.write(uploaded_resumes[fname])

# Fetch latest JD
with sqlite3.connect(DB_NAME) as conn:
    c = conn.cursor()
    c.execute("SELECT jd_text,jd_emb FROM job_description ORDER BY id DESC LIMIT 1")
    row = c.fetchone()
jd_text = row[0]
jd_emb = np.frombuffer(row[1], dtype=np.float32).reshape(1, -1)

# Precompute embeddings
resume_files = glob.glob(os.path.join(resumes_dir, "*"))
resume_embeddings = {}
resume_texts = {}

for path in resume_files:
    resume_json = parse_resume(path)
    resume_texts[path] = resume_json
    resume_embeddings[path] = embedder.encode([resume_json["FullText"]])[0]

# Evaluate all resumes
results = []
for path in resume_files:
    resume_emb = resume_embeddings[path]
    resume_json = resume_texts[path]

    resume_words = set(resume_json["FullText"].split())
    jd_words = set(jd_text.split())
    keyword_score = len(resume_words & jd_words)/max(len(jd_words),1)
    semantic_score = float(cosine_similarity([resume_emb],[jd_emb[0]])[0][0])

    final_score = (keyword_score*0.5 + semantic_score*0.5)*100
    missing_skills = [s for s in SKILLS_LIST if s in jd_text and s not in resume_json["FullText"]]
    verdict = "High" if final_score>=75 else "Medium" if final_score>=50 else "Low"
    feedback = f"Score: {final_score:.1f}. "+("✅ Great match!" if verdict=="High" else "⚠️ Partial match." if verdict=="Medium" else "❌ Low alignment.")
    if missing_skills: feedback += f" Missing: {', '.join(missing_skills[:10])}."

    with sqlite3.connect(DB_NAME) as conn:
        c = conn.cursor()
        c.execute("INSERT INTO evaluations (resume_name,score,verdict,missing_skills,feedback) VALUES (?,?,?,?,?)",
                  (os.path.basename(path), final_score, verdict, json.dumps(missing_skills), feedback))
        conn.commit()

    results.append({
        "Resume": os.path.basename(path),
        "Score": round(final_score,2),
        "Verdict": verdict,
        "Missing Skills": missing_skills,
        "Feedback": feedback
    })

df = pd.DataFrame(results)
display(df)
df.to_csv("optimized_batch_resume_results.csv", index=False)
print("✅ Batch evaluation completed. Results saved to 'optimized_batch_resume_results.csv'.")